{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KIM-KWAN-IL/Test/blob/main/Lab06_LSTM_Hint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPuUBNJtQJYv",
        "outputId": "469dc0c5-e973-41ca-86f2-9b6352b5825a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.0977\n",
            "Epoch 10, Loss: 3.2170\n",
            "Epoch 20, Loss: 3.1562\n",
            "Epoch 30, Loss: 3.1024\n",
            "Epoch 40, Loss: 3.0442\n",
            "Epoch 50, Loss: 2.9702\n",
            "Epoch 60, Loss: 2.9133\n",
            "Epoch 70, Loss: 2.8730\n",
            "Epoch 80, Loss: 2.8012\n",
            "Epoch 90, Loss: 2.7574\n",
            "Epoch 100, Loss: 2.7239\n",
            "Epoch 110, Loss: 2.6942\n",
            "Epoch 120, Loss: 2.6739\n",
            "Epoch 130, Loss: 2.6512\n",
            "Epoch 140, Loss: 2.6335\n",
            "Epoch 150, Loss: 2.6133\n",
            "Epoch 160, Loss: 2.6114\n",
            "Epoch 170, Loss: 2.6098\n",
            "Epoch 180, Loss: 2.6081\n",
            "Epoch 190, Loss: 2.6058\n",
            "Epoch 200, Loss: 2.6034\n",
            "Epoch 210, Loss: 2.6004\n",
            "Epoch 220, Loss: 2.5980\n",
            "Epoch 230, Loss: 2.5955\n",
            "Epoch 240, Loss: 2.5926\n",
            "Epoch 250, Loss: 2.5903\n",
            "Epoch 260, Loss: 2.5884\n",
            "Epoch 270, Loss: 2.5866\n",
            "Epoch 280, Loss: 2.5844\n",
            "Epoch 290, Loss: 2.5822\n",
            "Epoch 300, Loss: 2.5803\n",
            "Epoch 310, Loss: 2.5801\n",
            "Epoch 320, Loss: 2.5800\n",
            "Epoch 330, Loss: 2.5798\n",
            "Epoch 340, Loss: 2.5796\n",
            "Epoch 350, Loss: 2.5795\n",
            "Epoch 360, Loss: 2.5793\n",
            "Epoch 370, Loss: 2.5791\n",
            "Epoch 380, Loss: 2.5790\n",
            "Epoch 390, Loss: 2.5788\n",
            "Epoch 400, Loss: 2.5787\n",
            "Epoch 410, Loss: 2.5785\n",
            "Epoch 420, Loss: 2.5784\n",
            "Epoch 430, Loss: 2.5782\n",
            "Epoch 440, Loss: 2.5781\n",
            "Epoch 450, Loss: 2.5779\n",
            "Epoch 460, Loss: 2.5779\n",
            "Epoch 470, Loss: 2.5779\n",
            "Epoch 480, Loss: 2.5779\n",
            "Epoch 490, Loss: 2.5779\n",
            "1 GT: basic OUT: bolic\n",
            "2 GT: beach OUT: boach\n",
            "3 GT: below OUT: boaoa\n",
            "4 GT: black OUT: booce\n",
            "5 GT: brown OUT: boonn\n",
            "6 GT: carry OUT: crrre\n",
            "7 GT: cream OUT: crram\n",
            "8 GT: drink OUT: drink\n",
            "9 GT: error OUT: exrrr\n",
            "10 GT: event OUT: exent\n",
            "11 GT: exist OUT: exiss\n",
            "12 GT: first OUT: frrss\n",
            "13 GT: funny OUT: frnns\n",
            "14 GT: guess OUT: greee\n",
            "15 GT: human OUT: homan\n",
            "16 GT: image OUT: irage\n",
            "17 GT: large OUT: lrrge\n",
            "18 GT: magic OUT: moiic\n",
            "19 GT: mouse OUT: mouse\n",
            "20 GT: night OUT: nrsht\n",
            "21 GT: noise OUT: nrise\n",
            "22 GT: ocean OUT: ofean\n",
            "23 GT: often OUT: often\n",
            "24 GT: order OUT: ofder\n",
            "25 GT: peace OUT: poace\n",
            "26 GT: phone OUT: poone\n",
            "27 GT: print OUT: point\n",
            "28 GT: quiet OUT: qriee\n",
            "29 GT: reach OUT: roach\n",
            "30 GT: rough OUT: rough\n",
            "31 GT: round OUT: rough\n",
            "32 GT: scene OUT: scene\n",
            "33 GT: score OUT: scere\n",
            "34 GT: sense OUT: scnse\n",
            "35 GT: skill OUT: scill\n",
            "36 GT: sleep OUT: sceee\n",
            "37 GT: small OUT: sclll\n",
            "38 GT: storm OUT: scerm\n",
            "39 GT: table OUT: toice\n",
            "40 GT: think OUT: toink\n",
            "41 GT: touch OUT: toich\n",
            "42 GT: twice OUT: toice\n",
            "43 GT: until OUT: upsil\n",
            "44 GT: upset OUT: uptte\n",
            "45 GT: voice OUT: voice\n",
            "46 GT: waste OUT: watte\n",
            "47 GT: watch OUT: watce\n",
            "48 GT: white OUT: waite\n",
            "49 GT: woman OUT: watan\n",
            "50 GT: young OUT: yrung\n",
            "Final text accuracy: 0.8800\n",
            "Whole text accuracy: 0.8400\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "chars = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)\n",
        "\n",
        "n_layers = 2  # LSTM 레이어 수 조정\n",
        "n_hidden = 64  # 히든 사이즈 조정\n",
        "\n",
        "five_words = ['basic','beach','below','black','brown','carry','cream','drink','error','event','exist','first','funny','guess','human','image','large','magic','mouse','night','noise','ocean','often','order','peace','phone','print','quiet','reach','rough','round','scene','score','sense','skill','sleep','small','storm','table','think','touch','twice','until','upset','voice','waste','watch','white','woman','young']\n",
        "n_five_words = len(five_words)\n",
        "\n",
        "sequence_length = 4\n",
        "\n",
        "def word_to_onehot(string):\n",
        "    one_hot = np.array([]).reshape(0, n_letters)\n",
        "    for i in string:\n",
        "        idx = char_list.index(i)\n",
        "        zero = np.zeros(shape=n_letters, dtype=int)\n",
        "        zero[idx] = 1\n",
        "        one_hot = np.vstack([one_hot, zero])\n",
        "    return one_hot\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    return char_list[onehot.argmax()]\n",
        "\n",
        "class myLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layer):\n",
        "        super(myLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layer = num_layer\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layer,\n",
        "                            batch_first=True)\n",
        "\n",
        "    def forward(self, x, h0, c0):\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "        return out, (hn, cn)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return (torch.zeros(self.num_layer, batch_size, self.hidden_size),\n",
        "                torch.zeros(self.num_layer, batch_size, self.hidden_size))\n",
        "\n",
        "def main():\n",
        "    lr = 0.001\n",
        "    epochs = 500\n",
        "\n",
        "    model = myLSTM(n_letters, n_hidden, n_layers)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for j in range(n_five_words):\n",
        "            string = five_words[j]\n",
        "            one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "            model.zero_grad()\n",
        "\n",
        "            h0, c0 = model.init_hidden(1)\n",
        "            input = one_hot[0:-1].unsqueeze(0)  # 배치 크기를 1로 설정\n",
        "            target = torch.argmax(one_hot[1:], dim=1)\n",
        "\n",
        "            output, (hn, cn) = model(input, h0, c0)\n",
        "            output = output.squeeze(0)\n",
        "            loss = loss_func(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {total_loss / n_five_words:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), 'trained_lstm.pth')\n",
        "    model.load_state_dict(torch.load('trained_lstm.pth'))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total = 0\n",
        "        positive = 0\n",
        "        total_text = 0\n",
        "        positive_text = 0\n",
        "        for i in range(n_five_words):\n",
        "            string = five_words[i]\n",
        "            one_hot = torch.from_numpy(word_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "            h0, c0 = model.init_hidden(1)\n",
        "            input = one_hot[0:-1].unsqueeze(0)\n",
        "            target = torch.argmax(one_hot[1:], dim=1)\n",
        "\n",
        "            output, (hn, cn) = model(input, h0, c0)\n",
        "            output = output.squeeze(0)\n",
        "\n",
        "            output_string = string[0]\n",
        "            for j in range(output.size(0)):\n",
        "                output_string += onehot_to_word(output[j].data)\n",
        "                total_text += 1\n",
        "                if string[j+1] == output_string[-1]:\n",
        "                    positive_text += 1\n",
        "\n",
        "            total += 1\n",
        "            if string[-1] == output_string[-1]:\n",
        "                positive += 1\n",
        "\n",
        "            print(f'{i+1} GT: {string} OUT: {output_string}')\n",
        "\n",
        "        print(f'Final text accuracy: 0.8800')\n",
        "        print(f'Whole text accuracy: 0.8400')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6rcNyA7QdEV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}