{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KIM-KWAN-IL/Test/blob/main/DL_Lab3_Example_Code_logistic_regresstion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngkCeWTJIWWP"
      },
      "source": [
        "# Example 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = [[80,220],[75,167],[86,210],[110,330],[95,280],[67,190],[79,210],[98,250]]\n",
        "y_data = [[1],[0],[1],[1],[1],[0],[0],[1]]\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "print('x_train size =', x_train.shape)\n",
        "print('y_train size =', y_train.shape)\n",
        "\n",
        "W = torch.zeros((2,1), requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "print('------------------------------')\n",
        "print('< Before Train >')\n",
        "print('W size =', W)\n",
        "print('b size =', b)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr= 0.00001)\n",
        "\n",
        "for e in range(1000):\n",
        "  optimizer.zero_grad()\n",
        "  #hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))\n",
        "  hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
        "  #loss = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))\n",
        "  #cost = loss.mean()\n",
        "  cost = F.binary_cross_entropy(hypothesis, y_train) # Softmax included\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if e % 100 == 0:\n",
        "    print('epoch:%d cost:%f'%(e, cost))\n",
        "\n",
        "\n",
        "print('------------------------------')\n",
        "print('< After Train >')\n",
        "print('W = ', W)\n",
        "print('b =', b)\n",
        "\n",
        "prediction = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))\n",
        "# hypothsis 는 0과 1사이의 실수값이므로, 그 값이 0.5 보다 크면 1, 작으면 0 ==> binary classification\n",
        "correct_prediction = prediction.float() == y_train\n",
        "\n",
        "\n",
        "print('------------------------------')\n",
        "print('< Evaluation >')\n",
        "print(prediction)\n",
        "print(correct_prediction)"
      ],
      "metadata": {
        "id": "RG-h_aVcL1Dv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96498ae8-3d5c-436e-de61-498ef9cd77f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train size = torch.Size([8, 2])\n",
            "y_train size = torch.Size([8, 1])\n",
            "------------------------------\n",
            "< Before Train >\n",
            "W size = tensor([[0.],\n",
            "        [0.]], requires_grad=True)\n",
            "b size = tensor([0.], requires_grad=True)\n",
            "epoch:0 cost:0.693147\n",
            "epoch:100 cost:0.618535\n",
            "epoch:200 cost:0.617527\n",
            "epoch:300 cost:0.616542\n",
            "epoch:400 cost:0.615580\n",
            "epoch:500 cost:0.614641\n",
            "epoch:600 cost:0.613723\n",
            "epoch:700 cost:0.612827\n",
            "epoch:800 cost:0.611952\n",
            "epoch:900 cost:0.611097\n",
            "------------------------------\n",
            "< After Train >\n",
            "W =  tensor([[-0.0079],\n",
            "        [ 0.0063]], requires_grad=True)\n",
            "b = tensor([-0.0006], requires_grad=True)\n",
            "------------------------------\n",
            "< Evaluation >\n",
            "tensor([[0.6800],\n",
            "        [0.6127],\n",
            "        [0.6554],\n",
            "        [0.7703],\n",
            "        [0.7337],\n",
            "        [0.6609],\n",
            "        [0.6678],\n",
            "        [0.6900]], grad_fn=<MulBackward0>)\n",
            "tensor([[False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 1 with Higher Implementation"
      ],
      "metadata": {
        "id": "mQLybI-PhKxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = [[80,220],[75,167],[86,210],[110,330],[95,280],[67,190],[79,210],[98,250]]\n",
        "y_data = [[1],[0],[1],[1],[1],[0],[0],[1]]\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "print('x_train size =', x_train.shape)\n",
        "print('y_train size =', y_train.shape)\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BinaryClassifier, self).__init__()\n",
        "    self.linear = nn.Linear(2,1) # 입력차원 : 2, 출력차원: 1\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.sigmoid(self.linear(x))\n",
        "\n",
        "model = BinaryClassifier() # 모델 초기화\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr= 1e-6) # optimizer 설정\n",
        "\n",
        "for e in range(1000):\n",
        "  hypothesis = model(x_train)\n",
        "  cost = F.binary_cross_entropy(hypothesis, y_train) # Softmax included\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if e % 100 == 0:\n",
        "    print('epoch:%d cost:%f'%(e, cost))\n",
        "\n",
        "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "# hypothsis 는 0과 1사이의 실수값이므로, 그 값이 0.5 보다 크면 1, 작으면 0 ==> binary classification\n",
        "correct_prediction = prediction.float() == y_train\n",
        "\n",
        "print('------------------------------')\n",
        "print('< Evaluation >')\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ykegEGZiKL2",
        "outputId": "09aea6cd-8010-4fc6-e4da-e9c7035fc657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train size = torch.Size([8, 2])\n",
            "y_train size = torch.Size([8, 1])\n",
            "epoch:0 cost:3.497896\n",
            "epoch:100 cost:1.171866\n",
            "epoch:200 cost:0.711561\n",
            "epoch:300 cost:0.689296\n",
            "epoch:400 cost:0.687891\n",
            "epoch:500 cost:0.687617\n",
            "epoch:600 cost:0.687413\n",
            "epoch:700 cost:0.687214\n",
            "epoch:800 cost:0.687016\n",
            "epoch:900 cost:0.686818\n",
            "------------------------------\n",
            "< Evaluation >\n",
            "tensor([[True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classification Example"
      ],
      "metadata": {
        "id": "0Jx_LZ1qxXoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
        "y_data = [[0], [0], [0], [1], [1], [1]] # False False False True True  True\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "W = torch.zeros((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=1) # SGD 사용\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    # Cost 계산\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번 마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d} / {} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))\n",
        "\n",
        "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "prediction = hypothesis >= torch.FloatTensor([0.5]) #  0.5를 넘으면 True, 넘지 않으면 False\n",
        "correct_prediction = prediction.float() == y_train\n",
        "\n",
        "print(hypothesis)\n",
        "print(prediction)\n",
        "print(correct_prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWbyvNaKxd3n",
        "outputId": "9e6f97e5-5f86-4d5b-f62c-65012719461c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0 / 1000 Cost: 0.693147\n",
            "Epoch  100 / 1000 Cost: 0.134722\n",
            "Epoch  200 / 1000 Cost: 0.080643\n",
            "Epoch  300 / 1000 Cost: 0.057900\n",
            "Epoch  400 / 1000 Cost: 0.045300\n",
            "Epoch  500 / 1000 Cost: 0.037261\n",
            "Epoch  600 / 1000 Cost: 0.031672\n",
            "Epoch  700 / 1000 Cost: 0.027556\n",
            "Epoch  800 / 1000 Cost: 0.024394\n",
            "Epoch  900 / 1000 Cost: 0.021888\n",
            "Epoch 1000 / 1000 Cost: 0.019852\n",
            "tensor([[2.7648e-04],\n",
            "        [3.1608e-02],\n",
            "        [3.8977e-02],\n",
            "        [9.5622e-01],\n",
            "        [9.9823e-01],\n",
            "        [9.9969e-01]], grad_fn=<SigmoidBackward0>)\n",
            "tensor([[False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [ True],\n",
            "        [ True]])\n",
            "tensor([[True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True],\n",
            "        [True]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLM--XbFIY-h"
      },
      "source": [
        "# Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkSyOmGgIbJR",
        "outputId": "b53f4df9-f17e-4f76-b69c-501a27d86c24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train size = torch.Size([8, 3])\n",
            "y_train size = torch.Size([8])\n",
            "W size = torch.Size([3, 4])\n",
            "b size = torch.Size([1])\n",
            "epoch:0 cost:1.386294\n",
            "epoch:1000 cost:1.380785\n",
            "epoch:2000 cost:1.379236\n",
            "epoch:3000 cost:1.377279\n",
            "epoch:4000 cost:1.375425\n",
            "epoch:5000 cost:1.380006\n",
            "epoch:6000 cost:1.372731\n",
            "epoch:7000 cost:1.380072\n",
            "epoch:8000 cost:1.382767\n",
            "epoch:9000 cost:1.380724\n",
            "< After Train >\n",
            "W =  tensor([[-1.5915e-04, -5.0109e-03,  4.4209e-05,  4.6111e-05],\n",
            "        [ 4.4413e-05, -3.4068e-02,  1.4390e-04,  7.5486e-05],\n",
            "        [ 2.1288e-03,  2.0382e-03,  2.1328e-03,  2.6649e-03]],\n",
            "       requires_grad=True)\n",
            "b = tensor([-0.0002], requires_grad=True)\n",
            "---------\n",
            "< Evaluation >\n",
            "tensor([[1.0000, 0.9929, 1.0000, 1.0000],\n",
            "        [0.9999, 0.9572, 0.9999, 1.0000],\n",
            "        [1.0000, 0.9995, 1.0000, 1.0000],\n",
            "        [1.0000, 0.9986, 1.0000, 1.0000],\n",
            "        [1.0000, 0.9996, 1.0000, 1.0000],\n",
            "        [1.0000, 0.9991, 1.0000, 1.0000],\n",
            "        [1.0000, 0.9334, 1.0000, 1.0000],\n",
            "        [1.0000, 0.9966, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = [[80, 220, 6300], # 2 - cheetah\n",
        "          [75, 167, 4500], # 3 - jaguar\n",
        "          [86, 210, 7500], # 1 - leopard\n",
        "          [110, 330, 9000], # 0 - tiger\n",
        "          [95, 280, 8700], # 0 - tiger\n",
        "          [67, 190, 6800], # 3 - jaguar\n",
        "          [79, 210, 5000], # 2 - cheetah\n",
        "          [98, 250, 7200]] # 1 - leopard\n",
        "\n",
        "y_data = [2, 3, 1, 0, 0, 3, 2, 1]\n",
        "\n",
        "nb_class = 4 # tiger, leopard, cheetah, jaguar\n",
        "classes = {0: \"Tiger\", 1: \"Leopard\", 2: \"Cheetah\", 3: \"Jaguar\"}\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.LongTensor(y_data)\n",
        "\n",
        "print('x_train size =', x_train.shape)\n",
        "print('y_train size =', y_train.shape)\n",
        "\n",
        "W = torch.zeros((3, nb_class), requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True) # nb_class --> need to used multi perceptron\n",
        "\n",
        "print('W size =', W.shape)\n",
        "print('b size =', b.shape)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 0.00001)\n",
        "\n",
        "for e in range(10000):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = torch.sigmoid(x_train.matmul(W)+b) # Activation Function\n",
        "  cost = F.cross_entropy(hypothesis, y_train) # Softmax included\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if e % 1000 == 0:\n",
        "    print('epoch:%d cost:%f'%(e, cost))\n",
        "\n",
        "print('< After Train >')\n",
        "print('W = ', W)\n",
        "print('b =', b)\n",
        "\n",
        "prediction = torch.sigmoid(x_train.matmul(W) + b)\n",
        "\n",
        "print('---------')\n",
        "print('< Evaluation >')\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Possible Solution 1"
      ],
      "metadata": {
        "id": "ia3Zn2YKQHFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(8, 4)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))\n",
        "\n",
        "model = BinaryClassifier()\n",
        "\n",
        "x_data = [[80, 220, 6300], # 2 - cheetah\n",
        "          [75, 167, 4500], # 3 - jaguar\n",
        "          [86, 210, 7500], # 1 - leopard\n",
        "          [110, 330, 9000], # 0 - tiger\n",
        "          [95, 280, 8700], # 0 - tiger\n",
        "          [67, 190, 6800], # 3 - jaguar\n",
        "          [79, 210, 5000], # 2 - cheetah\n",
        "          [98, 250, 7200]] # 1 - leopard\n",
        "\n",
        "y_data = [2, 3, 1, 0, 0, 3, 2, 1]\n",
        "y_one_hot = [[0, 0, 1, 0],\n",
        "             [0, 0, 0, 1],\n",
        "             [0, 1, 0, 0],\n",
        "             [1, 0, 0, 0],\n",
        "             [1, 0, 0, 0],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 0, 1, 0],\n",
        "             [0, 1, 0, 0]\n",
        "             ]\n",
        "\n",
        "nb_class = 4 # tiger, leopard, cheetah, jaguar\n",
        "classes = {0: \"Tiger\", 1: \"Leopard\", 2: \"Cheetah\", 3: \"Jaguar\"}\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_one_hot)\n",
        "\n",
        "print('x_train size =', x_train.shape)\n",
        "print('y_train size =', y_train.shape)\n",
        "\n",
        "W = torch.zeros((3, nb_class), requires_grad = True)\n",
        "b = torch.zeros(nb_class, requires_grad = True) # nb_class --> need to used multi perceptron\n",
        "\n",
        "print('W size =', W.shape)\n",
        "print('b size =', b.shape)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr = 1e-6)\n",
        "\n",
        "for e in range(10000):\n",
        "  optimizer.zero_grad()\n",
        "  hypothesis = torch.sigmoid(x_train.matmul(W)+b) # Activation Function\n",
        "  cost = F.cross_entropy(hypothesis, y_train) # Softmax included\n",
        "  cost.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if e % 1000 == 0:\n",
        "    print('epoch:%d cost:%f'%(e, cost))\n",
        "\n",
        "print('< After Train >')\n",
        "print('W = ', W)\n",
        "print('b =', b)\n",
        "\n",
        "prediction = torch.sigmoid(x_train.matmul(W) + b)\n",
        "\n",
        "print('---------')\n",
        "print('< Evaluation >')\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VPaLbTmQIvu",
        "outputId": "81469608-bfa8-4a47-a090-d663736fd612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train size = torch.Size([8, 3])\n",
            "y_train size = torch.Size([8, 4])\n",
            "W size = torch.Size([3, 4])\n",
            "b size = torch.Size([4])\n",
            "epoch:0 cost:1.386294\n",
            "epoch:1000 cost:1.294630\n",
            "epoch:2000 cost:1.293651\n",
            "epoch:3000 cost:1.292665\n",
            "epoch:4000 cost:1.291668\n",
            "epoch:5000 cost:1.290663\n",
            "epoch:6000 cost:1.289652\n",
            "epoch:7000 cost:1.288638\n",
            "epoch:8000 cost:1.287622\n",
            "epoch:9000 cost:1.286609\n",
            "< After Train >\n",
            "W =  tensor([[-3.5448e-03, -3.0706e-04,  1.8038e-05,  3.2536e-03],\n",
            "        [ 1.6219e-03, -8.2642e-03,  7.1303e-05,  2.2952e-03],\n",
            "        [ 1.5829e-04,  4.0599e-04,  1.6471e-03, -4.1040e-04]],\n",
            "       requires_grad=True)\n",
            "b = tensor([-1.1345e-04, -3.9175e-05,  2.5380e-07,  5.9185e-05],\n",
            "       requires_grad=True)\n",
            "---------\n",
            "< Evaluation >\n",
            "tensor([[0.7447, 0.6715, 1.0000, 0.1394],\n",
            "        [0.6720, 0.6044, 0.9994, 0.2280],\n",
            "        [0.7726, 0.7830, 1.0000, 0.0898],\n",
            "        [0.8278, 0.7095, 1.0000, 0.0706],\n",
            "        [0.8167, 0.7665, 1.0000, 0.0679],\n",
            "        [0.7590, 0.7631, 1.0000, 0.1056],\n",
            "        [0.7010, 0.5671, 0.9997, 0.2120],\n",
            "        [0.7681, 0.6957, 1.0000, 0.1128]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}